{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![image](../../Utilities/tensorflow-logo.png)\n",
    "\n",
    "__Author: Christian Urcuqui__\n",
    "\n",
    "__Date: 5 September 2018__\n",
    "\n",
    "__Last updated: 5 September 2018__\n",
    "\n",
    "\n",
    "This notebook is oriented to understand TensorFlow, some of the examples and explanations were taken from the references (books and websites). \n",
    "\n",
    "\n",
    "# Understanding Tensorflow\n",
    "\n",
    "TensorFlow is an open source machine learning library for research and production, it is recognized due its implementations in _deep learning_. This project was proposed for high numerical computation. It has a flexible architecture for the deployment across a variety of platforms(i.e. CPUs, GPUs, TPUs), and from desktops to cluster of servers to mobile \n",
    "\n",
    "This project was developed by the Google Brain team and it was realized at the year 2015.\n",
    "\n",
    "https://www.tensorflow.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe width=\"626\" height=\"352\" src=\"https://www.youtube.com/embed/mWl45NkFBOc\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<iframe width=\"626\" height=\"352\" src=\"https://www.youtube.com/embed/mWl45NkFBOc\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will review Tensorflow in order to understand its main components and how it works, so it is divided in the next sections:\n",
    "\n",
    "+ [Computation Graphs](#Computation-Graphs)\n",
    "+ [Graphs, Sessions, and Fetches](#Graphs,-Sessions,-and-Fetches)\n",
    "+ [Matrix multiplication](#Matrix-multiplication)\n",
    "+ [Names](#Names)\n",
    "+ [References](#References)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation Graphs \n",
    "\n",
    "TensorFlow integrates the concept of computing operations that interact with one another through the application of dataflow graphs.\n",
    "\n",
    "The graph concept is a simple abstraction of nodes interconnected by edges, this concept is applicated in different contexts, for example network communications, neuronal networks and other kind of situations. \n",
    "\n",
    "In a _dataflow graph_, the edges allow data to \"flow\" from one node to another in a directed manner. On the other hand, each of the graph's nodes represents an operation, possibly applied to some input, and can generate an output that is transmitted on to other nodes. \n",
    "\n",
    "Operations in the graph include all kinds of functions, from simple arithmetic ones such as multiplication to more complex ones.\n",
    "\n",
    "Each graph has its own set of node dependencies and these relationships are classified as _direct_ and _indirect_. Being able to locate dependencies between units of our model allows us to both distribute computations across available resources and avoid performing redundant processes. \n",
    "\n",
    "\"A Graph contains a set of tf.Operation objects, which represent units of computation; and tf.Tensor objects, which represent the units of data that flow between operations\"[3].\n",
    "\n",
    "A Tensor is a N-dimensional vector, we can call (1000x3x3) as the shape or dimension of the resulting Tensor. Tensors can either be a constant or a variable\n",
    "\n",
    "__Nodes are operations, edges are tensor objects__\n",
    "\n",
    "\n",
    "<img src=\"../../Utilities/tensor.png\" width=\"500\">\n",
    "\n",
    "TensorFlow has these components, but first we have a skeleton graph. At this point no actual data flows in it and no computations take place. When we run the session, data enters the graph and computations occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs, Sessions, and Fetches\n",
    "\n",
    "A TensorFlow process involves two main phases:\n",
    "\n",
    "+ Make a graph \n",
    "+ Execute the graph\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import the TensorFlow package, we are going to assign the reconized alias \"tf\"\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the import method of TensorFlow we implicitly declared a default graph. In the next example we will create six nodes and the content of these variables should be regarded as the output of the operations, and not the opeartions themselves. \n",
    "\n",
    "Three of the nodes are going to be constants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =  tf.constant(5)\n",
    "b =  tf.constant(2)\n",
    "c =  tf.constant(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the last three nodes gets two existing variables as inputs, and performs simple arithmetic operations on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d =  tf.multiply(a,b)\n",
    "e =  tf.add(c, b)\n",
    "f =  tf.subtract(d,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the last example, we defined our first graph, a sequence of three constants that are the input of three arithmetic operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the graph through the TensorBoard, in order to d.call this program we must make the logs with the next code, let's see that this new file is in the main package of this .ipynb. Once we have this file we need to use it through the next command \n",
    "```\n",
    "tensorboard --logdir\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('.')\n",
    "writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the graph result of the six nodes declared\n",
    "\n",
    "<img src=\"../../Utilities/tensorboard.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once, we have finished our graph the next step is to run the computations that it represents, to do this we must create and run a session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outs =5\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    outs = sess.run(f)\n",
    "    sess.close()\n",
    "print(\"outs ={}\".format(outs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the last code, through the Session object, specifically, it's _run()_ method allows us to compute all the nodes in the graph, all the computing nodes were executed according to the set of dependencies. Once, the process is finished we need to close the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make another example of graph flow computation \n",
    "a = tf.constant(1)\n",
    "b = tf.constant(2)\n",
    "\n",
    "d = tf.add(a,b)\n",
    "c = tf.multiply(a,b)\n",
    "\n",
    "f = tf.add(c,d)\n",
    "e = tf.subtract(d,c)\n",
    "\n",
    "g = tf.div(f,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('.')\n",
    "writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create additional graphs and control their association with some given operations _tf.graph()_ creates a new graph, represented as a TensorFlow object. We are going to see that we can have two graphs at the same time in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.framework.ops.Graph object at 0x0000022DFEFA4748>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.framework.ops.Graph object at 0x0000022D865B2CF8>\n"
     ]
    }
   ],
   "source": [
    "g =  tf.Graph()\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look that we have two graphs with different identification, also the graph _g_ is not associated to the default graph. Let's see the associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "a =  tf.constant(5)\n",
    "\n",
    "print(a.graph is g)\n",
    "print(a.graph is tf.get_default_graph())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Fetches_ is the argument that is corresponding to the elements of the graph we wish to compute. Then, the idea is to use them through the _session.run()_ method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outs=[1, 2, 2, 3, 1, 5]\n",
      "<class 'numpy.int32'>\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    fetches = [a,b,c,d,e,f]\n",
    "    outs = session.run(fetches)\n",
    "print(\"outs={}\".format(outs))\n",
    "print(type(outs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look that each fetch element is a representation of a NumPy object, a material that I previously presented in this repository. If you want to know more information about this package for scientific computing with Python look this URL \n",
    "```\n",
    "https://github.com/urcuqui/Data-Science/blob/master/Exploratory%20Data%20Analysis/Numpy.ipynb\n",
    "```\n",
    "[Numpy](../../Exploratory%20Data%20Analysis/Numpy.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_8:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "c = tf.constant(4.0)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw, when we used _tf.constant_ we made a node with the corresponding passed value. Look that the object's type is a _Tensor_.\n",
    "\n",
    "Another important feature is the Tensor's attributes, look that they are necessary, but, in some applications (for example, image processing) we must pay attention to these variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casting\n",
    "\n",
    "It is an important to make sure our data types match throughout the graph, if we want to change the data type setting of a Tensor object, we can use the tf.cast() function, passing the relevtant Tensor and the new data type of interest as the firtst and second arguements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([1,2,3], name=\"x\", dtype=tf.float32)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'int64'>\n"
     ]
    }
   ],
   "source": [
    "x = tf.cast(x, tf.int64)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*WBopnJ1NgGbsatnFuUrNow.png\" width=\"450px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next steps in TensorFlow are going to be necessary to use random-number generators, we can make them from a _normal distribution_ through the `tf.random_normal()`, passing the shape, mean, and standard deviation as the first, second, and third arguments, respectively. Another two examples for useful random initializers are the _truncated normal_ where the values below and above two standard deviations from the main do not take, and the _uniform_ that samples values uniformly within some interval [a,b)\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../../Utilities/distribution.png\" width=\"500\">\n",
    "\n",
    "http://blog.cloudera.com/blog/2015/12/common-probability-distributions-the-data-scientists-crib-sheet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A feature to use when we want to explore the data content of an object is `tf.InteractiveSession()`, through it and the `.eval()` method, we can get a full look at the values without the need to constantly refer to the session object.\n",
    "\n",
    "`tf.InteractiveSession()` allows us to replace the usual `tf.Session()`, so we dond't need a variable holding the session for running ops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The content of 'c':\n",
      " [0. 1. 2. 3. 4.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tf.linscape allows us to make a ndarray of numbers n evenly spaced from a to b\n",
    "sess =  tf.InteractiveSession()\n",
    "c = tf.linspace(0.0, 4.0, 5)\n",
    "print(\"The content of 'c':\\n {}\\n\".format(c.eval()))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix multiplication\n",
    "\n",
    "This is an useful operation performs in TensorFlow via the `tf.matmul(A,B)` function for two Tensor objects A and B. It is important to anote that both Tensors must have the same number of dimensions and that they are aligned correctly with respecto to the intended multiplication. \n",
    "\n",
    "Let's do the next product:\n",
    "\n",
    "$Ax=b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "A = tf.constant([ [1,2,3], [4,5,6]])\n",
    "print(A.get_shape())\n",
    "\n",
    "x =  tf.constant([1,0,1])\n",
    "\n",
    "print(x.get_shape())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to multiply them, we need to add a dimension to _x_, transforming it from a 1D vector to a 2D single-column matrix. We can add another dimension through the Tensor's method `tf.expand_dims()`, together with the position of the added as the second arguement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "[[1]\n",
      " [0]\n",
      " [1]]\n",
      "matmul result:\n",
      " [[ 4]\n",
      " [10]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.expand_dims(x,1)\n",
    "print(x.get_shape())\n",
    "\n",
    "b = tf.matmul(A, x)\n",
    "\n",
    "sess =  tf.InteractiveSession()\n",
    "print(x.eval())\n",
    "\n",
    "print(\"matmul result:\\n {}\".format(b.eval()))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'transpose_1:0' shape=(1, 2) dtype=int32>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we want to flip an array, we do it with the next function\n",
    "print(b.get_shape())\n",
    "tf.transpose(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Names\n",
    "\n",
    "\n",
    "Each Tensor object has an identifiying name, this identification is a string. As with _dtype_, we can use the .name attribute to see the name of the object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:0\n",
      "c_1:0\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    c1 = tf.constant(4, dtype=tf.float64,name='c')\n",
    "    c2 = tf.constant(4, dtype=tf.int32,name='c')\n",
    "    \n",
    "print(c1.name)\n",
    "print(c2.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes when dealing with a large, complicated graph, we would like to make some node grouping to make it easier to follow and manage. We do so by using `tf.name_scoe(\"prefix\")` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:0\n",
      "prefix_name/c:0\n",
      "prefix_name/c_1:0\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    c1 = tf.constant(4, dtype=tf.float64, name='c')\n",
    "    with tf.name_scope(\"prefix_name\"):\n",
    "        c2 = tf.constant(4,dtype=tf.int32,name='c')\n",
    "        c3 = tf.constant(4,dtype=tf.float64,name='c')\n",
    "        \n",
    "print(c1.name)\n",
    "print(c2.name)\n",
    "print(c3.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prefixes are especially useful when we would like to divide a graph into subgraphs\n",
    "with some semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables, Placeholders, and Simple Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "The optimization process tunes the parameters of some given model. For that porporse, TensorFlow uses special objects called _Variables_. Unlike other Tensor objects that are \"refilled\" with data each time we run the session. __Variables maintain a fixed state in the graph__.\n",
    "\n",
    "We must call the `tf.Variable()` function to make a Variable and define what value it will be initialized with. Next, we have to explicitly perform an initialization operation by running the session with the `tf.global_variables_initializer()` method, which allocates the memory for the Variable and sets its initial values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre run: \n",
      "<tf.Variable 'var_1:0' shape=(1, 5) dtype=float32_ref>\n",
      "\n",
      "post run: \n",
      "[[ 0.16173129 -0.91304344  1.0675926   1.3795998  -1.156251  ]]\n"
     ]
    }
   ],
   "source": [
    "init_val = tf.random_normal((1,5),0,1)\n",
    "var = tf.Variable(init_val, name='var')\n",
    "print(\"pre run: \\n{}\".format(var))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    post_var = sess.run(var)\n",
    "print(\"\\npost run: \\n{}\".format(post_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run the code again a new variable is going to appear, as indicated by the automatic concatenation of `_1` to its name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders\n",
    "\n",
    "Placeholders can be thought of as empty Variables that will be filled with data later on. We use them by first constructing our graph and only when it is executed them with the input data. Placeholders have an optional shape arguement. If a shape is not fed or is passed as _None_, then the placeholder can be fed with data of any size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_1:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ph = tf.placeholder(tf.float32, shape=(None,10))\n",
    "ph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must feed it with some input values or else an exception will be thrown. The input data is passed to the `session.run()` method as a dictionary, where each key corresponds to a placeholder variable name, and the matching values are the data values.\n",
    "\n",
    "```\n",
    "with tf.Session as sess:\n",
    "    sess.run(s, feed_dict={x: X_data, w: w_data})\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outs = 0.6382045745849609\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_data =  np.random.randn(5,10)\n",
    "w_data = np.random.randn(10,1)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=(5,10))\n",
    "    w = tf.placeholder(tf.float32, shape=(10,1))\n",
    "    b = tf.fill((5,1), -1.)\n",
    "    xw = tf.matmul(x,w)\n",
    "    \n",
    "    xwb = xw + b\n",
    "    s = tf.reduce_max(xwb)\n",
    "    with tf.Session() as sess:\n",
    "        outs = sess.run(s, feed_dict={x: x_data, w: w_data})\n",
    "        \n",
    "print(\"outs = {}\".format(outs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "### Traning to predict\n",
    "\n",
    "We want to predict the variable _y_, which we want to esxplain using some feature vector _x_. According to a data science process the idea is to evaluate different models, in this case we are going to evaluate one. Our training data points will be used for \"tuning\" the model so that it best captures the desired relation. \n",
    "\n",
    "Our selected model is a regression model:\n",
    "\n",
    "$ f(x_{i}) = W^Tx_{i} + b$\n",
    "\n",
    "$ y_{i} = f(x_{i}) + \\epsilon_{i} $\n",
    "\n",
    "$ f(x_{i}) $ is a linear combination of some input data $ x_{i} $, with a set of weights $w$ and an intercept $b$. Our target output $ y_{i} $ is a noisy version of $ f(x_{i}) $ after being summed with Gaussian noise $ \\epsilon_{i} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =  tf.placeholder(tf.float32,shape=[None,3])\n",
    "y_true = tf.placeholder(tf.float32, shape=None)\n",
    "w = tf.Variable([[0,0,0]], dtype=tf.float32, name='weights')\n",
    "b = tf.Variable(0, dtype=tf.float32, name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred =  tf.matmul(w, tf.transpose(x)) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a loss function\n",
    "\n",
    "To capture the discrepancy between our model's predictions and the observed targets, we need a measure reflecting \"distance\". __This distance is often referred to as an _objective_ or a _loss function_, and we optimize the model by finding the set of parameters (weights and bias in this case) that minimize it.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE and cross entropy\n",
    "\n",
    "MSE (Mean squared error) is the most used loss, where for all samples we average the squared distances between the real target and what our model predicts across samples.\n",
    "\n",
    "$ L(y,yˆ) = \\frac{1}{N} \\sum_{i=1}^{n} (y_{i} - yˆ_{i})^{2} $\n",
    "\n",
    "This loss minimizes the mean square difference between an observed value an the model's fitted value (these differences are referred to as _residuals_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "+ Hope, T., Resheff, Y. S., & Lieder, I. (2017). Learning TensorFlow: A Guide to Building Deep Learning Systems. \" O'Reilly Media, Inc.\".\n",
    "\n",
    "+ https://towardsdatascience.com/a-beginner-introduction-to-tensorflow-part-1-6d139e038278\n",
    "\n",
    "+ https://www.tensorflow.org/api_docs/python/tf/Graph\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
