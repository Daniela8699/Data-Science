{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Author:__ Christian Urcuqui\n",
    "\n",
    "__Date:__ 06 September 2018\n",
    "\n",
    "__Last update:__ 06 September 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GPUs\n",
    "\n",
    "\n",
    "Sometimes a system has multiple computing devices. In TensorFlow, the supported device types are:\n",
    "\n",
    "+ _\"/cpu:0\"_: The CPU of your machine.\n",
    "+ _\"/device:GPU:0\"_: The GPU of your machine, if you have one.\n",
    "+ _\"/device:GPU:1\"_ The second GPU of your machine, etc.\n",
    "\n",
    "If TensorFlow has both CPU and GPU devices will be taken with more priority. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It creates a graph \n",
    "a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c= tf.matmul(a, b)\n",
    "# We will create a session with log_device_placement set to True\n",
    "sess = tf.Session(conf=tf.ConfigProto(log_device_placement=True))\n",
    "# Next, it runs the operation\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual device placement \n",
    "\n",
    "If you want to have more control of what is the device to run, you can use it with _tf.device_ to create a device context such that all the operations within that context will have the same device assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It creates a graph\n",
    "with tf.device('/cpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "# It creates a session with log_device_placement set to True\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Next, it runs the operation\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw __a__ and __b__ are assigned to _cpu:0_. Since a device was not assigned for the Matmul operation, but TensorFlow runtime will choose one based on the operation and available devices(for this case _gpu0_) and automatically copy tensors between devices if they are requiered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allowing GPU memory growth\n",
    "\n",
    "By default, TensorFlow maps nearly all of the GPU memory of all GPUs visible to the process. This is done to more efficiently use the relative precious GPU memory resources on the devices by reducing memory fragmentation. \n",
    "\n",
    "Sometimes it is desirable for the process to only allocate a subset of the available memory, or to only grow the memory usage as is needed by the process. In order to manage these resources, TensorFLow provides two Config options on the Session.\n",
    "\n",
    "+ __allow_growth__: it allocates only as much GPU memory based on runtime allocations. We can choose this option in the ConfigPro.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## References\n",
    "\n",
    "+ https://www.tensorflow.org/guide/using_gpu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
