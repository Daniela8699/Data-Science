{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author: Christian Urcuqui**\n",
    "\n",
    "**Date: 3 August 2018**\n",
    "\n",
    "# notMNIST\n",
    "\n",
    "This is an example to process a notMNIST dataset, it is integrated by handwritten digits, the MNSIT (Mixed National Institute of Standards and Technology) is one of the most image dataset used in image processing and machine learning. \n",
    "We could say that notMNIST is a good example to say \"hello world\" in tensorflow because a lot of examples are available and some of them took this dataset as the first steps in deep learning, such as, since the application of a simple neural network with a softmax function until the application of a ConvNet architecture.\n",
    "\n",
    "\n",
    "<img src=\"../../Utilities/notmnist.jpeg\" width=\"250\">\n",
    "\n",
    "In this Jupyter notebook I will explore different examples in order to know what is the best solution, for that the notebook's content is divided en these sections:\n",
    "\n",
    "- [Libraries](#Libraries)\n",
    "- [Dataset](#Dataset)\n",
    "- [Classical-ML](#Classical-ML)\n",
    "- [Udemy-DeepLearning](#Udemy-DeepLearning)\n",
    "- [Book](#Book)\n",
    "- [References](#References).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To resolve this problem I used two example sources, one from the book [1] and the last one from the Udemy Course [2].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "+ **imageio 2.2.0**, it is python library that provides an easy interace to read an write a wide range of image data.\n",
    "+ **matploit**, is a specially library to make plots \n",
    "+ **numpy**, it is a library make math and scientific operations \n",
    "+ **tarfile**, it allows use to process tar files\n",
    "+ **IPython**, it is a library from Jupyter\n",
    "+ **six**, it is another Python library, the idea of this is for smoothing over the differences beetween the Python versions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "The next code will download the dataset to our local machine. The repository has characters rendered in a variety of fonts on 28x28 image, moreover, it only has the label images since the 'A' to the 'J', specifically, 10 classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download: notMNIST_large.tar.gz\n",
      "0%....5%....10%....15%....20%....25%....30%....35%....40%....45%....50%....55%....60%....65%....70%....75%....80%....85%....90%....95%....100%\n",
      "Download Complete!\n",
      "Found and verified .\\notMNIST_large.tar.gz\n",
      "Attempting to download: notMNIST_small.tar.gz\n",
      "0%....5%....10%....15%....20%....25%....30%....35%....40%....45%....50%....55%....60%....65%....70%....75%....80%....85%....90%....95%....100%\n",
      "Download Complete!\n",
      "Found and verified .\\notMNIST_small.tar.gz\n"
     ]
    }
   ],
   "source": [
    "url = 'https://commondatastorage.googleapis.com/books1000/'\n",
    "last_percent_reported = None\n",
    "data_root = '.' # Change me to store data elsewhere\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "  \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "  slow internet connections. Reports every 5% change in download progress.\n",
    "  \"\"\"\n",
    "  global last_percent_reported\n",
    "  percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "  if last_percent_reported != percent:\n",
    "    if percent % 5 == 0:\n",
    "      sys.stdout.write(\"%s%%\" % percent)\n",
    "      sys.stdout.flush()\n",
    "    else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent\n",
    "        \n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  dest_filename = os.path.join(data_root, filename)\n",
    "  if force or not os.path.exists(dest_filename):\n",
    "    print('Attempting to download:', filename) \n",
    "    filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "    print('\\nDownload Complete!')\n",
    "  statinfo = os.stat(dest_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', dest_filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
    "  return dest_filename\n",
    "\n",
    "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the dataset\n",
    "\n",
    "Once the datasets are downloaded, the next code will extract these files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data for .\\notMNIST_large. This may take a while. Please wait.\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall(data_root)\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way is to download the data from the tensorflow repository  (in its examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-9d38aaa07213>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From D:\\Usuarios\\rhaps\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From D:\\Usuarios\\rhaps\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Usuarios\\rhaps\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Usuarios\\rhaps\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /tmp/data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From D:\\Usuarios\\rhaps\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '/tmp/data'\n",
    "NUM_STEPS= 1000\n",
    "MINIBATCH_SIZE = 100\n",
    "\n",
    "data = input_data.read_data_sets(DATA_DIR, one_hot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Udemy \n",
    "\n",
    "\"Let's take a peek at some of the data to make sure it looks sensible. Each exemple should be an image of a character A through J rendered in a different font. Display a sample of the images that we just downloaded. Hint: you can use the package IPython.display.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x000002B30DF7CF60>\n",
      "(55000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(data.train)\n",
    "\n",
    "print(data.train.images.shape) ## 550000 points for training with of 28 by 28 pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data in a more manageable format\n",
    "\n",
    "The next code will format the entire dataset into a 3D array (image index, x,y) of floating point values, its normalized will have aproximately zero mean and standard deviation ~0.5 and it let the training easier down the road. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book\n",
    "\n",
    "The first step that the book focuses is to unroll the image pixels as a single long vector denoted x:\n",
    "<br>\n",
    "\n",
    "$xw^0 = \\sum_{}x_{i}w_{i}^0$\n",
    "<br>\n",
    "\n",
    "The idea is apply an algorithm to transform each pixel's wights in a range of 0 and 1 using the softmax function, next, with the sufficient evidence for each of the 10 possible digits. The final assigment will be the digit which accumulates the most evidence:\n",
    "\n",
    "$digit=argmax(xW)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.9%\n"
     ]
    }
   ],
   "source": [
    "# In the placeholder could be supplied when triggering it. Moreover, the variable is an element manipulated by the computation.\n",
    "# The image is a placeholder because it could be supplied by us when running the computation graph.\n",
    "# each image is of size 724 (28*28 pixels unrolled in into a single vector)\n",
    "\n",
    "# The size [None,784] menas that each image of size 784, and None is an indicator that we are not currently specificyng how many of these images\n",
    "# we will use at once\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "\n",
    "# we have 10 digits, same like the last two code lines, we are specifiying a placeholder to the 10 digits\n",
    "y_true = tf.placeholder(tf.float32, [None, 10])\n",
    "# we are going to use the function matmul to multiply to matrixes\n",
    "y_pred = tf.matmul(x, W)\n",
    "\n",
    "# note that in the book was used a deprecated method of softmax cross entropy. \n",
    "# Cross_entropy is the measure of similarity - a natural choice when the model outputs probabilities - this element is\n",
    "# usually called the loss function - the softmax function - \n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_pred, labels=y_true))\n",
    "\n",
    "# The next step is how we are going to minimize the loss function, for this case we will use the gradient descent approach\n",
    "# 0.5 is the learning rate \n",
    "gd_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "# This is the evaluation proccedure \n",
    "correct_mask = tf.equal(tf.argmax(y_pred,1), tf.argmax(y_true,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_mask, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # The next line will intialize all variables \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #train process with the number of steps and the minibatch size\n",
    "    for _ in range(NUM_STEPS):\n",
    "        # the next line gets a subset of the data \n",
    "        batch_xs, batch_ys = data.train.next_batch(MINIBATCH_SIZE)\n",
    "        # the session run will use all the process in order to train our neural network\n",
    "        sess.run(gd_step, feed_dict={x: batch_xs, y_true: batch_ys})\n",
    "\n",
    "    # the results of our testing will be saved in the accuracy variable\n",
    "    ans = sess.run(accuracy, feed_dict={x: data.test.images, y_true: data.test.labels})\n",
    "\n",
    "print (\"Accuracy: {:.4}%\".format(ans*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow tutorial for begginers\n",
    "\n",
    "The page of tensorflow has some tutorials that allow any person to know how the theory is apply while the code is used. So, I would like to take some parts of them in order make some conclusions of it.\n",
    "\n",
    "### Softmax Regression\n",
    "\n",
    "In the tensor flow tutorial were used a softmax regression function like the book's example, but, they included the bias variable \n",
    "\n",
    "$Evidence = \\sum_{j}W_{i,j}x_{j}+b_{i}$\n",
    "\n",
    "The index $j$ represents each pixel in the image x, and  $i$  is the class. They explain that when the NN has enough evidence of the target, the idea is use the softmax function in order to get its probabilities \n",
    "\n",
    "$y=Softmax(Evidence)$\n",
    "\n",
    "$y=Softmax(Wx+b)$\n",
    "\n",
    "In another notebook I commented about the softmax function and its use in this area, also, the next image from the tutorial allow us to understand how is the architecture of the NN.\n",
    "\n",
    "<image src=\"https://www.tensorflow.org/images/softmax-regression-scalargraph.png\" height=\"250\" weight=\"250\">\n",
    "    \n",
    "### Cross entropy\n",
    "\n",
    "Cross entropy is the way to calculate the cost or the lost of our model, in other words it allows us to understand how far is the model's results to the desire outcomes. This is the function:\n",
    "\n",
    "\n",
    "$H_{y'}(y) = - \\sum_{i}y'_{i}log(y_{i})$\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the solution proposed in the tutorial\n",
    "\n",
    "x = tf.placeholder(tf.float32,[None, 784])\n",
    "\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "tf.nn.softmax(tf.matmul(W,x) + b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] Hope, T., Resheff, Y. S., & Lieder, I. (2017). Learning TensorFlow: A Guide to Building Deep Learning Systems. \" O'Reilly Media, Inc.\".\n",
    "\n",
    "[2] Udemy \n",
    "\n",
    "[3] https://www.tensorflow.org/versions/r1.2/get_started/mnist/beginners"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
